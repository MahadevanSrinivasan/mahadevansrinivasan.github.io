<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Logistic Regression Theory</title>
  <meta name="description" content="Several years ago, I took multiple Machine Learning related courses on Coursera. And I did a quite a few projects on those courses. I don’t really have the r...">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/2016/08/29/logistic-regression.html">
  <link rel="alternate" type="application/rss+xml" title="The Shy Bulb" href="http://localhost:4000/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">The Shy Bulb</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/projects/">Projects</a>
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Logistic Regression Theory</h1>
    <p class="post-meta">Aug 29, 2016</p>
  </header>

  <article class="post-content">
    <p>Several years ago, I took multiple Machine Learning related courses on Coursera. And I did a quite a few projects on those courses. I don’t really have the results or the code documented anywhere. So, I am starting a small Machine Learning series to help me also recollect all those projects. One of the first projects we did on the Coursera ML course is Digit Recognition. There is a well known digit dataset called MNIST. For the course though, we used a small subset of the dataset. We have a set of scanned images which contain numbers and we have to design a model to correctly classify them. Before we look into the full classification, it makes sense to start with something simpler. How about just classifying the 0s and the 1s?</p>

<h3 id="theory">Theory</h3>

<p>Logistic Regression for classification works on a very simple principle. Let us say, we have 10 features and 2 classes. We want to create a model that can take in an input of 10 features and output which class it belongs to. Going back to our digit classification example, if given the image of a 0, the model should output “Class 1” and for the image of a 1, it should output “Class 2”. What are the features of an image? Easiest one is pixels. So, if we have a 20x20 image, then we have 400 features.</p>

<p>For convenience, we represent the image as one long vector. In our example, it is just a vector of 400 values. Going back to the model, imagine another vector of size 400. Let us call it \( \theta \). We want to select values of \( \theta \) that when multiplied with the pixels of a “0” image outputs a value of 0 and when multiplied with the pixels of a “1” image outputs a value of 1. Simple enough. How do we mathematically formulate this?</p>

<p>We know that the pixels (assuming a gray scale image) have values between 0 and 255. Let us say \( \theta \) can take any real value. How do we restrict the product of these two vectors to have just two values 0 and 1?</p>

<h4 id="sigmoid-function">Sigmoid function</h4>

<p>That is where we need a sigmoid function. A sigmoid function takes in a real value and outputs a value between 0 and 1. So, if we were to send the product of \( \theta \) and the Image pixels through a sigmoid function, we can create a real number output between 0 and 1. Now, we could simply choose “Class 1” if the value is below 0.5 and “Class 2” if the value is above 0.5 breaking ties for 0.5 however you want.</p>

<script type="math/tex; mode=display">g =  \frac{1.0}{(1.0 + e^{-z})}</script>

<h3 id="octave-code">Octave code</h3>

<figure class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="n">num_labels</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>

<span class="nb">load</span><span class="p">(</span><span class="s1">'ex3data1_01.mat'</span><span class="p">);</span> <span class="c1">% training data stored in arrays X, y</span>

<span class="n">lambda</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">;</span>
<span class="c1">% Some useful variables</span>
<span class="n">m</span> <span class="o">=</span> <span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>

<span class="n">all_theta</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>

<span class="c1">% Add ones to the X data matrix</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="nb">ones</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="n">X</span><span class="p">];</span>

<span class="n">initial_theta</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

<span class="n">options</span> <span class="o">=</span> <span class="nb">optimset</span><span class="p">(</span><span class="s1">'GradObj'</span><span class="p">,</span> <span class="s1">'on'</span><span class="p">,</span> <span class="s1">'MaxIter'</span><span class="p">,</span> <span class="mi">50</span><span class="p">);</span>

<span class="k">for</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">num_labels</span>
    <span class="n">all_theta</span><span class="p">(</span><span class="n">c</span><span class="p">,:)</span> <span class="o">=</span> <span class="n">fmincg</span> <span class="p">(</span><span class="o">@</span><span class="p">(</span><span class="n">t</span><span class="p">)(</span><span class="n">lrCostFunction</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">),</span> <span class="n">lambda</span><span class="p">)),</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span>
<span class="k">end</span>
<span class="c1">% Prediction</span>
<span class="p">[</span><span class="o">~</span><span class="p">,</span> <span class="n">pred</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">all_theta</span> <span class="o">*</span> <span class="n">X</span><span class="o">'</span><span class="p">,</span> <span class="p">[],</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(:);</span>

<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'\nTraining Set Accuracy: %f\n'</span><span class="p">,</span> <span class="nb">mean</span><span class="p">(</span><span class="nb">double</span><span class="p">(</span><span class="n">pred</span> <span class="o">==</span> <span class="n">y</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span><span class="p">);</span></code></pre></figure>

<p>First, we load the input data. Here I have taken the input data provided by Dr. Andrew Ng and reduced it to a smaller set with images for just 0 and 1. Next, we add a bias term to the input. So, the input array X will have 401 columns after this step. If the number of samples is <code class="highlighter-rouge">m</code>. Then X is an <code class="highlighter-rouge">m x 401</code> matrix. We start with an initial value of all zeroes for the feature vector \( \theta \). It’s size is <code class="highlighter-rouge">401 x 1</code>. Now, all we have to do is run an unconstrained optimization function which optimizes \( \theta \) to give the smallest cost function. Let us look at the cost function next.</p>

<h3 id="cost-function">Cost Function</h3>

<p>I recommend watching Dr. Ng’s lecture on how we came up with the cost function. I will give a quick intuition. Here is the equation:</p>

<p><script type="math/tex">J(\theta) = \frac{1}{m} \sum_{i=1}^m [-y^{(i)} log(h_{\theta}(x^{(i)}) - (1-y^{(i)} log(1 - h_{\theta}(x^{(i)}))]</script>
where
<script type="math/tex">h_{\theta}(x^{(i)}) = g(\Theta^{T}x^{(i)})</script></p>

<p>We have <code class="highlighter-rouge">m</code> training samples. <code class="highlighter-rouge">y</code> is the actual label for the image. If the current \( \theta \) predicts the <code class="highlighter-rouge">y</code> value correctly, then it should add nothing or a very small value to the cost function. But, if it predicts it wrongly it should add a large value to the cost function. Take for example an image with a zero. If \( h_{\theta}(x^{(i)}) \) turns out to be zero. Then it is a match. First term becomes zero because \( y^{(i)} \) is zero and the second term becomes zero because <code class="highlighter-rouge">log(1 - 0)</code> is zero. Similar reasoning holds good when we classify a 1 correctly. But, if we were to classify something wrong, we will end up increasing the cost function. We will also need to come up with a gradient function to be used with the unconstrained optimization function.</p>

<figure class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="k">function</span> <span class="p">[</span><span class="n">J</span><span class="p">,</span> <span class="n">grad</span><span class="p">]</span> <span class="o">=</span> <span class="n">lrCostFunction</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)</span>
<span class="c1">%LRCOSTFUNCTION Compute cost and gradient for logistic regression with</span>
<span class="c1">%regularization</span>
<span class="c1">%   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using</span>
<span class="c1">%   theta as the parameter for regularized logistic regression and the</span>
<span class="c1">%   gradient of the cost w.r.t. to the parameters.</span>

<span class="c1">% Initialize some useful values</span>
<span class="n">m</span> <span class="o">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c1">% number of training examples</span>

<span class="n">J</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">grad</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span>

<span class="n">J</span> <span class="o">=</span> <span class="p">(</span> <span class="p">(</span><span class="mi">1</span> <span class="p">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="s1">'*log(sigmoid(X*theta)) - (1-y)'</span><span class="o">*</span><span class="nb">log</span><span class="p">(</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="o">*</span><span class="n">theta</span><span class="p">)))</span> <span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda</span><span class="p">/(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span><span class="o">'*</span><span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">))</span> <span class="p">;</span>
<span class="n">htheta</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="o">*</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">;</span>
<span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="p">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span><span class="o">'</span> <span class="o">*</span> <span class="n">htheta</span><span class="p">);</span>
<span class="n">grad</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda</span> <span class="p">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">);</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(:);</span>

<span class="k">end</span></code></pre></figure>

<h3 id="final-thoughts">Final Thoughts</h3>

<p>When we try to just classify 0s and 1s, we can easily get to 100% accuracy on the training set. But, in real life we will need to run the test on a different (previously unpresented) dataset to measure the accuracy of our classifier. In the next post, we will look at how to extend this to a multi-class classification problem and some results.</p>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">The Shy Bulb</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>The Shy Bulb</li>
          <li><a href="mailto:macsdev {at} gmail {dot} com">macsdev {at} gmail {dot} com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/MahadevanSrinivasan">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">MahadevanSrinivasan</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/macsdev">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">macsdev</span>
            </a>
          </li>
          
        </ul>
      </div>

    </div>

  </div>
  <script type="text/javascript"
            src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

</footer>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-61562983-1', 'auto');
  ga('send', 'pageview');

</script>



  </body>

</html>
